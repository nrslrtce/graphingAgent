{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain import hub\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI (\n",
    "    api_key = 'ollama',\n",
    "    model = 'llama3.2:1b',\n",
    "    base_url = 'http://localhost:11434/v1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_wrapper = ArxivAPIWrapper (\n",
    "    top_k_results = 2,\n",
    "    doc_content_chars_max = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv = ArxivQueryRun (\n",
    "    api_wrapper = arxiv_wrapper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [arxiv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools (tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\nividhaRaut\\genericProject\\Lib\\site-packages\\langsmith\\client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull ('wfh/react-agent-executor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['{messages}'], input_types={'{messages}': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001AB24554540>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, metadata={'lc_hub_owner': 'wfh', 'lc_hub_repo': 'react-agent-executor', 'lc_hub_commit_hash': 'bccfbbc5de8559d19d44c8ea2229bb6d06c99e402ea29b8694b294a31730a7a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant.'), additional_kwargs={}), MessagesPlaceholder(variable_name='{messages}')])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_19872\\197613477.py:1: LangGraphDeprecationWarning: Parameter 'messages_modifier' in function 'create_react_agent' is deprecated as of version 0.1.9 and will be removed in version 0.3.0. Use 'state_modifier' parameter instead.\n",
      "  agent_executor = create_react_agent (llm_with_tools, tools, messages_modifier = prompt)\n"
     ]
    }
   ],
   "source": [
    "agent_executor = create_react_agent (llm_with_tools, tools, messages_modifier = prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke ({'messages' : [('user', 'What is in paper 1706.03762')]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is in paper 1706.03762', additional_kwargs={}, response_metadata={}, id='4878bb8e-93a2-4982-88af-b6453a3c499a'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_qspw1d6l', 'function': {'arguments': '{\"query\":\"paper 1706.03762\"}', 'name': 'arxiv'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 217, 'total_tokens': 240, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'llama3.2:1b', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5bf6e304-86ff-40e9-85a1-0474b17ada47-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'paper 1706.03762'}, 'id': 'call_qspw1d6l', 'type': 'tool_call'}], usage_metadata={'input_tokens': 217, 'output_tokens': 23, 'total_tokens': 240, 'input_token_details': {}, 'output_token_details': {}}),\n",
       " ToolMessage(content='Published: 2020-02-12\\nTitle: GLU Variants Improve Transformer\\nAuthors: Noam Shazeer\\nSummary: Gated Linear Units (arXiv:1612.08083) consist of the component-wise product\\nof two linear projections, one of which is first passed through a sigmoid\\nfunction. Variations on GLU are possible, using different nonlinear (or even\\nlinear) functions in place of sigmoid. We test these variants in the\\nfeed-forward sublayers of the Transformer (arXiv:1706.03762)\\nsequence-to-sequence model, and find that some of ', name='arxiv', id='2f7b6722-87b4-4f78-be09-265b0c532705', tool_call_id='call_qspw1d6l'),\n",
       " AIMessage(content='GLU Variants Improve Transformer.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 231, 'total_tokens': 239, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'llama3.2:1b', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None}, id='run-812b3110-df56-426b-95bf-a859a3a369d4-0', usage_metadata={'input_tokens': 231, 'output_tokens': 8, 'total_tokens': 239, 'input_token_details': {}, 'output_token_details': {}})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['messages']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genericProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
